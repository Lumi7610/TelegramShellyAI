import ollama
import chromadb # vector database for memory storage
from termcolor import colored# More readable

path = "AI_memory" # foldername for the vector database

client = chromadb.PersistentClient(path=path) # define and open folder path
client.heartbeat() # start chromadb
collection = client.get_or_create_collection("My_collection") # Get or create collection with name "My_collection"




# Modelfile of the new model
# FROM llama3 (existing llm)(Base of your new model)
# For modelfile use ''' '''
# Inside the modelfile use """ """
# Replace YOUR GGUF FILE PATH with your gguf file path

modelfile = '''
FROM /Users/haolunchong/VSCODE/YoutubeShelly/silicon-maid-7b.Q4_K_M (1).gguf

TEMPLATE """
<|im_start|>system
{{ .System }}
<|im_end|>
<|im_start|>User
{{ .Prompt }}
<|im_end|>
<|im_start|>Shelly
"""

PARAMETER temperature 1
PARAMETER stop <|im_start|>
PARAMETER stop <|im_end|>

SYSTEM """
You are a girl named Shelly, You like chocolates and you like to do sports.
You are very shy and not very talkative.
"""
'''


def messageShelly(prompt):



    collection.add(
        documents=[prompt], # Save prompt(user message) in documents
        metadatas=[{"user": "User"}], # save "User" as username of the sender
        ids=[str(collection.count() + 1)], # Save id as new id
    )



    # Make a new model using siliconmaid
    ollama.create(model="Shelly", modelfile=modelfile)

    # Get Shelly, put in the prompt and ask Shelly about the promt
    response = (ollama.generate(model="Shelly",prompt=prompt))['response'] 

    collection.add(
        documents=[response], # Save response(AI output) in documents
        metadatas=[{"user": "Shelly"}], # save "Shelly" as username of the sender
        ids=[str(collection.count() + 1)], # Save id as new id
    )


    return response



While True:    
    print(colored(collection.get(), "green")) # Print all the information in "My_collection"
    print(colored(messageShelly(input("User:")), "light_blue"))

